# Mothur analysis pipeline v0.1-21012018
## Pipeline developed by D. Giovannelli and M. Magliulo @ELSI and based on the pipeline of Cody Sheik from U Minnesota Duluth
## 21 January 2018

## NOTES
#The pipeline is optimized for our *polyphemus* server with 16 cores. Change the parameters accordingly.

##PIPELINE
#NOTES

#Extract the sequence and quality score data from your fastq files, create the reverse complement of the reverse read and then join the reads into contigs. Uncomment and use this option only if starting from multiple fastq.
#make.contigs(file=seqs.files, processors=16)

##Lets screen the sequences for homopolymers and any odd sequences that could mess things up in the downstream
screen.seqs(fasta=seqs.fasta, group=seqs.groups, maxambig=0, maxhomop=8, maxlength=300)

##Here we'll start the dereplication process, this makes the dataset smaller
unique.seqs(fasta=current)

##Create a table containing the counts for each unique sequences for each group
count.seqs(name=current, group=current)

##Align the seqs to the core Silva refDB database
align.seqs(flip=t, fasta=current, reference=/scratch/mothur_database/silva.seed_v132.align, processors=16)

##Summarize the current sequences
summary.seqs(fasta=current)

##Again screen any non-aligned sequence. Sometimes this can screen cool things out.
screen.seqs(fasta=current, count=current, optimize=start-end, criteria=90, processors=16)

##Silva uses a very large matrix to align the 16S. Lets remove any columns that aren't necessary
filter.seqs(fasta=current, vertical=T, trump=., processors=16)

##Filtering the sequences may create new redundancy not picked up in the previous pre-cluster. Again make the data smaller with unique and pre-cluster
unique.seqs(fasta=current, count=current)
pre.cluster(fasta=current, count=current, diffs=2)

##Chimeras happen through PCR and potentially the sequencing lets remove suspicious ones
chimera.vsearch(fasta=current, count=current, dereplicate=t)
remove.seqs(fasta=current, accnos=current, name=current)

##Cluster the seqs into OTUs using the new algorithm. This will cluster OTU at the 97% similarity cutoff. Please change it accordingly to your preference
cluster(fasta=current, count=current, processors= 16, cutoff=0.03)

##Create a shared file at our OTU cutoff. 0.03 is the standard for "Species" level OTUs.
make.shared(list=current, count=current, label=0.03)

##Lets get a set of representative sequences for each OTUs to generate the taxonomy
get.oturep(list=current, count=current, fasta=current, method=abundance)

##Here we classify the OTU reps to the SILVA database
classify.seqs(fasta=current, count=current, reference=/scratch/mothur_database/silva.nr_v132.align, taxonomy=/scratch/mothur_database/silva.nr_v132.tax, cutoff=60, output=simple, processors=16)
classify.otu(taxonomy= current, count= current, list= current)

#Remove the sequences that belong to Mitochondria and Chloroplast and Eukarya
remove.lineage(shared=current, constaxonomy=current, taxon=Mitochondria-Chloroplast-Eukaryota, label=0.03)

#Create the version 1.0 biom formatted table of the results. YOu may want to convert it to version 2,1 using the *biom convert* command later
make.biom(shared=current, groups=seqs.good.groups)
